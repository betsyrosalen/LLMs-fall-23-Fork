{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200cde67",
   "metadata": {},
   "source": [
    "## Tokenization lab\n",
    "LLM's and ChatGPT | Fall 2023 | McSweeney | CUNY Graduate Center\n",
    "\n",
    "**Due:** October 8\n",
    "\n",
    "\n",
    "### Background\n",
    "The purpose of this lab is to explore different tokenization methods. On their own, tokenization methods don't do much. However, they are the starting place for all natural language processing. \n",
    "\n",
    "\n",
    "#### Notes\n",
    "This is a short lab using the same dataset throughout. Feel free to switch it up, but once you are comfortable with how the different alogorithms approach the task of breaking up text, move on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47efac",
   "metadata": {},
   "source": [
    "You will be using the `datasets` package. You can [install the package](https://pypi.org/project/datasets/) with `$ pip install datasets`. If you do not have `pip` or `conda` installed on your machine, please install it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import timeit\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d2c62",
   "metadata": {},
   "source": [
    "The next cell is just downloading the dataset. You need to be connected to the internet for this to work. \n",
    "\n",
    "This dataset is hosted by [Hugging Face](https://huggingface.co). Hugging Face hosts machine learning models, datasets, and more. We will reference them again. It's a great place to find corpora. \n",
    "\n",
    "\n",
    "The dataset is called [American Stories](https://huggingface.co/datasets/dell-research-harvard/AmericanStories). Please skim the Dataset Card. All models and datasets on the Hugging Face hub have these associated cards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36427c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide what year you want between 1810 and 1963\n",
    "\n",
    "my_year = \"1960\"\n",
    "\n",
    "# Decide how many articles you want to work with (keep this small - it's slow)\n",
    "num_articles = 10\n",
    "\n",
    "#  Download data for your choice of year (1810 to 1963)\n",
    "dataset = load_dataset(\"dell-research-harvard/AmericanStories\",\n",
    "    \"subset_years\",\n",
    "    year_list=[my_year]\n",
    ")\n",
    "\n",
    "# Get the first n articles from that year\n",
    "# instantiate the counter\n",
    "i=0\n",
    "# instantiate the string\n",
    "my_articles = ''\n",
    "# loop through each article for that year\n",
    "for article in dataset[my_year]:\n",
    "    #the article is a dictionary, \n",
    "    #we're getting the text of the article by accessing the key, \"article\"\n",
    "    my_articles += article.get('article')\n",
    "    #add one to our counter\n",
    "    i+=1\n",
    "    #if the counter is greater than num_articles-1, stop looping\n",
    "    if i>(num_articles-1): break\n",
    "    \n",
    "#validate that it is what we expect by checking on first 100 characters\n",
    "print(my_articles[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d221a",
   "metadata": {},
   "source": [
    "This section is for formatting. It removes almost all the markup in these articles. It's a fairly standard set of character encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove new line and other formatting characters\n",
    "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
    "    my_articles = my_articles.replace(char, \" \")\n",
    "my_articles[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e522efd",
   "metadata": {},
   "source": [
    "# Whitespace tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e0c7f",
   "metadata": {},
   "source": [
    "First we'll just break up the words using whitespace. As noted in class, this is a really common first pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#this is a magic function to determine how long a cell takes to run. \n",
    "#It MUST be the first thing in a cell\n",
    "\n",
    "#split the whole string on spaces. This returns a list\n",
    "whitespace_tokens = my_articles.split(' ')\n",
    "\n",
    "#check the list\n",
    "whitespace_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb27496",
   "metadata": {},
   "source": [
    "Note: \"Âµs\" is microseconds, or a millionth of a second 1/1,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4bf8ea",
   "metadata": {},
   "source": [
    "# Morphological Tokenization \n",
    "\n",
    "Lemmatizing is the process of breaking down text into tokens by first breaking it up into \"words\" and then using syntactic knowledge of the language (in this case, English) to break up the words. \n",
    "\n",
    "Princeton maintains the [morphy project](https://wordnet.princeton.edu/documentation/morphy7wn#:~:text=Morphology%20in%20WordNet%20uses%20two,word%20that%20is%20in%20WordNet.), which powers `nltk`'s [WordNet Lemmatizer](https://www.nltk.org/api/nltk.stem.wordnet.html). You do NOT need to read this entire documentation, just acknowledge that it requires a significant amount of knowledge about English in order to make it work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This lemmatizer is based on the Morphy project above\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "#Uncomment these two lines - you may need to download these, maybe not. \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "wn_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16560507",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#first we have to split the string on spaces to get \"words\"\n",
    "whitespace_tokens = my_articles.split(' ')\n",
    "\n",
    "my_lemmas = []\n",
    "for word in whitespace_tokens:\n",
    "    w = wn_lemmatizer.lemmatize(word)\n",
    "    my_lemmas.append(w)\n",
    "my_lemmas[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883aa4ba",
   "metadata": {},
   "source": [
    "Notice how much time it takes to tokenize on whitespace versus using morphological rules. Also notice if it produced the output you expected. Sometimes it doesn't. \n",
    "\n",
    "ms is a millisecond, or one one thousandth of a second 1/1,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d79061",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f282d",
   "metadata": {},
   "source": [
    "There are two implementations of BPE here. The first [uses a package (bpe)](https://github.com/soaxelbrooke/python-bpe) that you will have to install using `pip` (see above). \n",
    "\n",
    "This will implement the algorithm we covered in class and that you can review at [Hugging Face](https://youtu.be/HEikzVL-lZU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1639f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe import Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "whitespace_tokens = my_articles.split(' ')\n",
    "\n",
    "# calling the Encoder algorithm\n",
    "# we've specified 100 token vocab and 95% to be tokenized\n",
    "# the other 5% is transformed into UNK\n",
    "encoder = Encoder(100, pct_bpe=0.95)\n",
    "encoder.fit(whitespace_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(encoder.tokenize(my_articles))\n",
    "\n",
    "print(next(encoder.inverse_transform(encoder.transform([my_articles]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90324f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13beaf38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
